
import os
import pandas as pd
from pathlib import Path
from snakemake.shell import shell

shell.executable("powershell.exe")
#shell.executable("C:/Program Files/PowerShell/7/pwsh.exe")

# info to provide on each test:
onstart:
    print("In rule 'evaluation table' remind to log:\nLLM utilized=\nnumber of times this LLM was used so far=\nnumber of input papers=")

Papers = glob_wildcards("workdir/inputs/{text}.md").text
original_paper = glob_wildcards("workdir/expected_output/or{antimony}.txt").antimony

rule all:
    input:
        expand("workdir/fabric_output/{sample}.txt", sample=Papers),
        expand("workdir/expected_output/or{model}.txt", model=original_paper),
        #expand("workdir/csv_output/{sample}_result.csv", sample=Papers),
        expand("workdir/final_models/{sample}_tested.txt", sample=Papers),
        "workdir/all_simulations.csv",
        "workdir/all_semantic_comparison.csv",
        "workdir/test_table.csv",
        "workdir/commit_version.txt"

rule conversion:
    input:
        "workdir/inputs/{sample}.md"

    output:
        "workdir/fabric_output/{sample}.txt"

    shell:
        "Type {input} | fabric -p Antimony_Converter > {output}"
        
        
#"Type {input} | fabric -p Antimony_Converter > {output}]" 
#Get-Content {input} | fabric -p Antimony_Converter | Set-Content {output}

rule first_decode:
    input:
        "workdir/fabric_output/{sample}.txt"  
    output:
        "workdir/decoded/{sample}_1dec.txt"
    run:
        with open(input[0], 'rb') as source_file:
            with open(output[0], 'w+b') as dest_file:
                contents = source_file.read()
                dest_file.write(contents.decode('utf-16').encode('utf-8'))
          

rule first_simulation:
    input:
        "workdir/decoded/{sample}_1dec.txt" 

    output: 
        "workdir/Error/{sample}_1er.txt",
        "workdir/simulated/{sample}_1S.txt"
    
    script:
        "scripts/check_simulation.py"

rule first_Evaluation:
    input:
        "workdir/Error/{sample}_1er.txt"
    
    output:
        "workdir/Evaluation/first/{sample}_1eval.csv"
    
    script:
        "scripts/Error_Evaluation_1.py"

rule first_aggregation:
    input:
        expand("workdir/Evaluation/first/{sample}_1eval.csv", sample=Papers)
    output:
        "workdir/CSV/first_evaluation.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)


rule first_correction:
    input:
        model = "workdir/simulated/{sample}_1S.txt",
        error = "workdir/Error/{sample}_1er.txt"

    output:
        corrected_model = "workdir/corrected/{sample}_1C.txt"

    shell:
        "(Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -p Antimony_Editor > {output.corrected_model}" 


rule second_decode:
   input:
        "workdir/corrected/{sample}_1C.txt"  
   output:
        "workdir/decoded/{sample}_2dec.txt"
   run:
        with open(input[0], 'rb') as source_file:
            with open(output[0], 'w+b') as dest_file:
                contents = source_file.read()
                dest_file.write(contents.decode('utf-16').encode('utf-8'))

rule second_simulation:
    input:
        "workdir/decoded/{sample}_2dec.txt" 

    output: 
        "workdir/Error/{sample}_2er.txt",
        "workdir/simulated/{sample}_2S.txt"
    
    script:
        "scripts/check_simulation.py"


rule second_Evaluation:
    input:
        "workdir/Error/{sample}_2er.txt"
    
    output:
        "workdir/Evaluation/second/{sample}_2eval.csv"
    
    script:
        "scripts/Error_Evaluation_2.py"

rule second_aggregation:
    input:
        expand("workdir/Evaluation/second/{sample}_2eval.csv",sample=Papers)
    output:
        "workdir/CSV/second_evaluation.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)

rule second_correction:
    input:
        model = "workdir/simulated/{sample}_2S.txt",
        error = "workdir/Error/{sample}_2er.txt"

    output:
        corrected_model = "workdir/corrected/{sample}_2C.txt"

    shell:
        "(Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -p Antimony_Editor > {output.corrected_model}" 
 

rule third_decode:
    input:
        "workdir/corrected/{sample}_2C.txt"  
    output:
        "workdir/decoded/{sample}_3dec.txt"
    run:
        with open(input[0], 'rb') as source_file:
            with open(output[0], 'w+b') as dest_file:
                contents = source_file.read()
                dest_file.write(contents.decode('utf-16').encode('utf-8'))

# third Simulation: the models coming from second correction will be copied in the ./final_model repository: it contains all tested model (corrected or not). In the end we are going to generate a final csv file with some info about each 
# model simulation (% of compiled and simulated model from first simulation without correction until the last one after the second correction).

rule third_simulation:
    input:
        "workdir/decoded/{sample}_3dec.txt" 

    output: 
        "workdir/Error/{sample}_3er.txt",
        "workdir/final_models/{sample}_tested.txt"
    
    script:
        "scripts/check_simulation.py"


rule third_Evaluation:
    input:
        "workdir/Error/{sample}_3er.txt"
    
    output:
        "workdir/Evaluation/third/{sample}_3eval.csv"
    
    script:
        "scripts/Error_Evaluation_3.py"

rule third_aggregation:
    input:
        expand("workdir/Evaluation/third/{sample}_3eval.csv",sample=Papers)
    output:
        "workdir/CSV/third_evaluation.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)

rule aggregate_all:
    input:
        expand("workdir/CSV/{number}_evaluation.csv", number=["first","second","third"])

    output:
        "workdir/all_simulations.csv"

    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=False, axis=1)
        df.to_csv(output[0], index=False)

rule semantic_evaluation_generated:
    input:
        "workdir/final_models/{sample}_tested.txt"
    
    output:
        "workdir/semantic_evaluation/generated/{sample}_G_semantic.csv"
    
    script:
        "scripts/semantic_evaluation.py"

rule semantic_evaluation_originals:
    input:
        "workdir/expected_output/or{model}.txt"
    
    output:
        "workdir/semantic_evaluation/original/{model}_O_semantic.csv"
    
    script:
        "scripts/semantic_evaluation.py"

rule merge_generated:
    input:
        expand("workdir/semantic_evaluation/generated/{sample}_G_semantic.csv",sample=Papers)
    output:
        "workdir/semantic_evaluation/Semantic_Outputs_generated.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=False, axis=0)
        df.to_csv(output[0], index=False)

rule merge_originals:
    input:
        expand("workdir/semantic_evaluation/original/{model}_O_semantic.csv", model=original_paper)
    output:
        "workdir/semantic_evaluation/Semantic_Outputs_original.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=False, axis=0)
        df.to_csv(output[0], index=False)

rule parameter_comparison:
    input:
        "workdir/semantic_evaluation/Semantic_Outputs_generated.csv",
        "workdir/semantic_evaluation/Semantic_Outputs_original.csv"
    
    output:
        "workdir/all_semantic_comparison.csv"
    
    script:
        "scripts/final_comparison.py"


rule evaluation_table:
    input:
        "workdir/all_simulations.csv",
        "workdir/all_semantic_comparison.csv"
    
    output:
        "workdir/test_table.csv"
    
    script:
        "scripts/test_table.py"

# recording the commited version of prompts, py scripts and snakefile for each test.

rule record_commit_version:
    output:
        "workdir/commit_version.txt"
    shell:
        "echo git-commit: (git rev-parse --short HEAD) > {output}"
