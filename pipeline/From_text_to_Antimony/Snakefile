
import os
import pandas as pd
from pathlib import Path
from snakemake.shell import shell

shell.executable("powershell.exe")
#shell.executable("C:/Program Files/PowerShell/7/pwsh.exe")

# info to provide on each test:
onstart:
    print("In rule 'evaluation table' remind to log:\nLLM utilized=\nday/time-slot where this LLM has been used so far=\nnumber of input papers=\n")

Papers = glob_wildcards("workdir/input_RMC/{text}.md").text

rule all:
    input:
        expand("workdir/fabric_output/{sample}.txt", sample=Papers), 
        expand("workdir/expected_output_RMC/or{sample}.txt", sample=Papers),
        #expand("workdir/csv_output/{sample}_result.csv", sample=Papers),
        expand("workdir/final_models/{sample}_tested.txt", sample=Papers),
        "workdir/all_simulations.csv",
        "workdir/all_semantic_comparison.csv",
        "workdir/test_table.csv"

rule conversion:
    input:
        "workdir/input_RMC/{sample}.md"

    output:
        "workdir/fabric_output/{sample}.txt"

    shell:
        "Type {input} | fabric -t 1 -T 1 -p Converter > {output}; Start-Sleep -Seconds 1.5"
        
# PS command for GPT models (like gpt-5) : Type {input} | fabric -t 1 -T 1 -p Antimony_Converter > {output}; Start-Sleep -Seconds 1.5
# PS command for claude models (like claude-sonnet-4.*) : Type {input} | fabric -t 0.2 -p Antimony_Converter > {output}; Start-Sleep -Seconds 1.5

rule first_decode:
    input:
        "workdir/fabric_output/{sample}.txt"  
    output:
        "workdir/decoded/{sample}_1dec.txt"
    run:
        with open(input[0], 'rb') as source_file:
            with open(output[0], 'w+b') as dest_file:
                contents = source_file.read()
                dest_file.write(contents.decode('utf-16').encode('utf-8'))
          

rule first_simulation:
    input:
        "workdir/decoded/{sample}_1dec.txt" 

    output: 
        "workdir/Error/{sample}_1er.txt",
        "workdir/simulated/{sample}_1S.txt"
    
    script:
        "scripts/check_simulation.py"

rule first_Evaluation:
    input:
        "workdir/Error/{sample}_1er.txt"
    
    output:
        "workdir/Evaluation/first/{sample}_1eval.csv"
    
    script:
        "scripts/Error_Evaluation_1.py"

rule first_aggregation:
    input:
        expand("workdir/Evaluation/first/{sample}_1eval.csv", sample=Papers)
    output:
        "workdir/CSV/first_evaluation.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)


rule first_correction:
    input:
        model = "workdir/simulated/{sample}_1S.txt",
        error = "workdir/Error/{sample}_1er.txt"

    output:
        corrected_model = "workdir/corrected/{sample}_1C.txt"

    shell:
        "(Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -t 1 -T 1 -p Editor > {output.corrected_model}; Start-Sleep -Seconds 1.5" 

# PS command for GPT models (like gpt-5) : (Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -t 1 -T 1 -p Antimony_Editor > {output.corrected_model}; Start-Sleep -Seconds 1.5
# PS command for claude models (like claude-sonnet-4.*) : (Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -t 0.2 -p Antimony_Editor > {output.corrected_model}; Start-Sleep -Seconds 1.5

rule second_decode:
   input:
        "workdir/corrected/{sample}_1C.txt"  
   output:
        "workdir/decoded/{sample}_2dec.txt"
   run:
        with open(input[0], 'rb') as source_file:
            with open(output[0], 'w+b') as dest_file:
                contents = source_file.read()
                dest_file.write(contents.decode('utf-16').encode('utf-8'))

rule second_simulation:
    input:
        "workdir/decoded/{sample}_2dec.txt" 

    output: 
        "workdir/Error/{sample}_2er.txt",
        "workdir/simulated/{sample}_2S.txt"
    
    script:
        "scripts/check_simulation.py"


rule second_Evaluation:
    input:
        "workdir/Error/{sample}_2er.txt"
    
    output:
        "workdir/Evaluation/second/{sample}_2eval.csv"
    
    script:
        "scripts/Error_Evaluation_2.py"

rule second_aggregation:
    input:
        expand("workdir/Evaluation/second/{sample}_2eval.csv",sample=Papers)
    output:
        "workdir/CSV/second_evaluation.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)

rule second_correction:
    input:
        model = "workdir/simulated/{sample}_2S.txt",
        error = "workdir/Error/{sample}_2er.txt"

    output:
        corrected_model = "workdir/corrected/{sample}_2C.txt"

    shell:
        "(Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -t 1 -T 1 -p Editor > {output.corrected_model}; Start-Sleep -Seconds 1.5" 

# PS command for GPT models (like gpt-5) : (Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -t 1 -T 1 -p Antimony_Editor > {output.corrected_model}; Start-Sleep -Seconds 1.5
# PS command for claude models (like claude-sonnet-4.*) : (Type {input.model}) + '==SIMULATION ERROR==' + (Type {input.error}) | fabric -t 0.2 -p Antimony_Editor > {output.corrected_model}; Start-Sleep -Seconds 1.5 

rule third_decode:
    input:
        "workdir/corrected/{sample}_2C.txt"  
    output:
        "workdir/decoded/{sample}_3dec.txt"
    run:
        with open(input[0], 'rb') as source_file:
            with open(output[0], 'w+b') as dest_file:
                contents = source_file.read()
                dest_file.write(contents.decode('utf-16').encode('utf-8'))

# third Simulation: the models coming from second correction will be copied in the ./final_model repository: it contains all tested model (corrected or not). In the end we are going to generate a final csv file with some info about each 
# model simulation (% of compiled and simulated model from first simulation without correction until the last one after the second correction).

rule third_simulation:
    input:
        "workdir/decoded/{sample}_3dec.txt" 

    output: 
        "workdir/Error/{sample}_3er.txt",
        "workdir/final_models/{sample}_tested.txt"
    
    script:
        "scripts/check_simulation.py"


rule third_Evaluation:
    input:
        "workdir/Error/{sample}_3er.txt"
    
    output:
        "workdir/Evaluation/third/{sample}_3eval.csv"
    
    script:
        "scripts/Error_Evaluation_3.py"

rule third_aggregation:
    input:
        expand("workdir/Evaluation/third/{sample}_3eval.csv",sample=Papers)
    output:
        "workdir/CSV/third_evaluation.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)

rule AAFE_Evaluation:
    input:
        "workdir/final_models/{sample}_tested.txt",
        "workdir/expected_output_RMC/or{sample}.txt"
    output:
        "workdir/Evaluation/AAFE/gen{sample}_vs_or{sample}_aafe_eval.csv",
        "workdir/Sim_plots/gen{sample}_vs_or{sample}.png"
    script:
        "scripts/AAFE_&_plots.py"

rule fourth_aggregation:
    input:
        expand("workdir/Evaluation/AAFE/gen{sample}_vs_or{sample}_aafe_eval.csv",sample=Papers)
    output:
        "workdir/CSV/AAFE_evaluation.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=True)
        df.to_csv(output[0], index=False)

rule aggregate_all:
    input:
        expand("workdir/CSV/{number}_evaluation.csv", number=["first","second","third", "AAFE"])

    output:
        "workdir/all_simulations.csv"

    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=False, axis=1)
        df.to_csv(output[0], index=False)

rule semantic_evaluation_generated:
    input:
        "workdir/final_models/{sample}_tested.txt"
    
    output:
        "workdir/semantic_evaluation/generated/{sample}_G_semantic.csv"
    
    script:
        "scripts/semantic_evaluation.py"

rule semantic_evaluation_originals:
    input:
        "workdir/expected_output_RMC/or{sample}.txt"
    
    output:
        "workdir/semantic_evaluation/original/{sample}_O_semantic.csv"
    
    script:
        "scripts/semantic_evaluation.py"

rule merge_generated:
    input:
        expand("workdir/semantic_evaluation/generated/{sample}_G_semantic.csv",sample=Papers)
    output:
        "workdir/semantic_evaluation/Semantic_Outputs_generated.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=False, axis=0)
        df.to_csv(output[0], index=False)

rule merge_originals:
    input:
        expand("workdir/semantic_evaluation/original/{sample}_O_semantic.csv", sample=Papers)
    output:
        "workdir/semantic_evaluation/Semantic_Outputs_original.csv"
    run:
        import pandas as pd
        
        dfs = [pd.read_csv(file) for file in input]
        df = pd.concat(dfs, ignore_index=False, axis=0)
        df.to_csv(output[0], index=False)

rule parameter_comparison:
    input:
        "workdir/semantic_evaluation/Semantic_Outputs_generated.csv",
        "workdir/semantic_evaluation/Semantic_Outputs_original.csv"
    
    output:
        "workdir/all_semantic_comparison.csv"
    
    script:
        "scripts/final_comparison.py"


rule evaluation_table:
    input:
        "workdir/all_simulations.csv",
        "workdir/all_semantic_comparison.csv"
    
    output:
        "workdir/test_table.csv"
    
    script:
        "scripts/test_table.py"

